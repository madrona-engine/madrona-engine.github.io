<!DOCTYPE html>
<html lang='en'>
  <head>
    <title>Madrona Engine</title>
    <meta charset='UTF-8'>
    <link rel='icon' type='image/x-icon' href='favicon.ico'>
    <meta name='viewport' content='width=device-width'>
    <link href='https://fonts.googleapis.com/css?family=JetBrains+Mono' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>
    <link rel='stylesheet' href='style.css'>
  </head>

  <body class='blog-background'>
    <div class='madrona-nav-wrapper blog-background'>
      <div class='madrona-nav'>
        <a href='/' class='logo-wrapper'>
          <img src='madrona_simple.png' width='50' />
          <span class='logo'>Madrona&nbsp;Engine</span>
        </a>
        <div class='madrona-nav-items'>
          <a href='/renderer-blog.html' class='link'>
            Blog
          </a>
          <a href='https://github.com/shacklettbp/madrona' class='gh'>
            <img src='github-mark.svg' alt='GitHub' />
          </a>
        </div>
      </div>
    </div>

    <div class='blog'>

    <section id='paper-title'>
      <a class='paper-title2' href='shacklett_siggraph23.pdf' target='_blank'>
        High-Throughput Batch Rendering for Embodied AI
      </a>
    <!--
      <div class='authors2'>
        <a href='https://www.linkedin.com/in/luc-rosenzweig/'>Luc&nbsp;Guy Rosenzweig</a>,
        <a href='https://cs.stanford.edu/~bps'>Brennan&nbsp;Shacklett</a>,
        <a href='https://www.linkedin.com/in/warren-xia-2a55641b6/'>Warren Xia</a>,
        <a href='http://graphics.stanford.edu/~kayvonf/'>Kayvon&nbsp;Fatahalian</a>
      </div>
      <p> SIGGRAPH Asia Conference 2024
      <br>
      <br>
      <br>
    -->
    </section>

    <div class='paper-block2'>
      <!--
      <div class='paper-title-wrapper'>
        <a class='paper-title' href='shacklett_siggraph23.pdf' target='_blank'>
          High-Throughput Batch Rendering for Embodied AI
        </a>
      </div>
      -->
      <div class='paper-details-wrapper'>
        <div id='paper-details' class='paper-details'>
          <div class='authors2'>
            <a href='https://www.linkedin.com/in/luc-rosenzweig/'>Luc&nbsp;Guy Rosenzweig</a>,
            <a href='https://cs.stanford.edu/~bps'>Brennan&nbsp;Shacklett</a>,
            <a href='https://www.linkedin.com/in/warren-xia-2a55641b6/'>Warren Xia</a>,
            <a href='http://graphics.stanford.edu/~kayvonf/'>Kayvon&nbsp;Fatahalian</a>
          </div>
          <div class='conference'>
            Proceedings of SIGGRAPH Asia 2024
          </div>
        </div>
        <div id='paper-download2'>
          <a href='madrona-renderer.pdf' target='_blank'>
            <span class='pdf-download-icon material-symbols-outlined'>description</span><div class='pdf-download-text'>PDF</div>
          </a>
        </div>
      </div>
    </div>



    <section id='renderer-paper' class='info-section'>
      <div class='content'>

        <h3>Abstract</h3>
        <p class='detail paper-abstract'>
          In this paper we study the problem of efficiently rendering images for embodied AI training workloads, where agent training involves rendering millions to billions of independent, low-resolution frames, often with simple lighting and shading, that serve as the agent's observations of the world. To enable high-throughput training from images, we design a flexible, batch-mode rendering interface that allows state-of-the-art GPU-accelerated batch world simulators to efficiently communicate with high-performance rendering backends. Using this interface we architect and compare two high-performance renderers: one based on the GPU hardware-accelerated graphics pipeline and a second based on a GPU software implementation of ray tracing. To evaluate these renderers and encourage further research by the graphics community in this area, we build a rendering benchmark for this under-explored regime. We find that the ray tracing renderer outperforms the rasterization-based solution across the benchmark on a datacenter-class GPU, while also performing competitively in geometrically complex environments on a high-end consumer GPU. When tasked to render large batches of independent 128x128 images, the ray tracer can exceed 100,000 frames per second per GPU for simple scenes, and exceed 10,000 frames per second per GPU on geometrically complex scenes from the HSSD dataset. 
        </p>

        <h3>Code</h3>
        <p class='detail paper-abstract'>
        The code for the renderer is available <a href='https://github.com/shacklettbp/madrona'>here</a> (specifically, you can find the <a href='https://github.com/shacklettbp/madrona/tree/main/src/mw/device'>ray tracing</a> and <a href='https://github.com/shacklettbp/madrona/tree/main/src/render'>rasterizer</a> code in the linked directories). You can also find a benchmarking repository to reproduce the results of the paper <a href='https://github.com/llGuy/madrona_benchmark'>here</a>.
        </p>

    <br>

    <h3> Visualization
    <section id='video'>
      <div id='main-video-container'>
        <video autoplay muted playsinline width='720' height='405' poster='renderer_thumbnail.jpg'>
          <source src='renderer.mp4' type='video/mp4' />
        </video>
        <button class='replay'><img src='replay.svg' alt='replay'></button>
      <p class="small emph">Example batch renderer outputs of the high geometry <a href="https://3dlg-hcvc.github.io/hssd/">HSSD</a> scenes.</p>
      </div>
    </section>

        <h3>Citation</h3>
        <pre class='citation'><code>@article{rosenzweig24madronarenderer,
    title   = {High-Throughput Batch Rendering for Embodied AI},
    author  = {Luc Guy Rosenzweig and Brennan Shacklett and
               Warren Xia and 
               Kayvon Fatahalian},
    conference = {SIGGRAPH Asia 2024 Conference Papers},
    year    = {2024}
}</code></pre>

      </div>
    </section>


    </div>
  </body>
</html>
